{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "783634b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from utils.schema import State, InputState\n",
    "\n",
    "# LLM cache\n",
    "from langchain_core.caches import InMemoryCache\n",
    "from langchain_core.globals import set_llm_cache\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "if not GROQ_API_KEY:\n",
    "    raise ValueError(\"Please set GROQ_API_KEY in your .env file\")\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ac29d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleHi(state: InputState):\n",
    "    \n",
    "    return {\"response\": \"Hello! how can I help you today?\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3dc6cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ZeroShot(state: InputState):\n",
    "    q = state.query\n",
    "    prompt_text = f\"\"\"You are an AI reasoning assistant.\n",
    "Solve the following problem directly.\n",
    "Do not explain steps. Do not show reasoning.\n",
    "\n",
    "Question: {q}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    response = llm.invoke(prompt_text).content\n",
    "    return {\"response\": response}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccf2337b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FewShot(state: InputState):\n",
    "    q = state.query\n",
    "    prompt_text = f'''You are an AI reasoning assistant.\n",
    "Look at the examples and follow the same style to solve the new problem.\n",
    "\n",
    "Examples:\n",
    "Question: If John has 2 apples and buys 3 more, how many does he have?\n",
    "Answer: 5\n",
    "\n",
    "Question: What is 12 + 8?\n",
    "Answer: 20\n",
    "\n",
    "Now solve this problem in the same way:\n",
    "\n",
    "Question: {q}\n",
    "Answer:'''\n",
    "    response = llm.invoke(prompt_text).content\n",
    "    return {\"response\": response}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ca2ef4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def COT(state: InputState):\n",
    "    q = state.query\n",
    "    prompt_text = f'''You are an AI reasoning assistant.\n",
    "Solve this problem carefully.\n",
    "Think step by step and show your reasoning before giving the final answer.\n",
    "\n",
    "Question: {q}\n",
    "\n",
    "Step-by-step reasoning:\n",
    "\n",
    "Final Answer:'''\n",
    "    response = llm.invoke(prompt_text).content\n",
    "    return {\"response\": response}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f46b380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_condition(state: InputState):\n",
    "    #return node IDs exactly as added below\n",
    "    if state.prompt == \"ZeroShot\":\n",
    "        return \"zero\"\n",
    "    elif state.prompt == \"FewShot\":\n",
    "        return \"few\"\n",
    "    else:\n",
    "        return \"cot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48813571",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"simple\", simpleHi)\n",
    "workflow.add_node(\"zero\", ZeroShot)\n",
    "workflow.add_node(\"few\", FewShot)\n",
    "workflow.add_node(\"cot\", COT)\n",
    "\n",
    "workflow.add_edge(START, \"simple\")\n",
    "workflow.add_conditional_edges(\"simple\", conditional_condition)\n",
    "workflow.add_edge(\"zero\", END)\n",
    "workflow.add_edge(\"few\", END)\n",
    "workflow.add_edge(\"cot\", END)\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebbc9b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find the total amount of milk produced per day, we need to multiply the number of cows by the amount of milk each cow produces per day.\n",
      "\n",
      "Step 1: Identify the number of cows - The farmer has 5 cows.\n",
      "Step 2: Identify the amount of milk each cow produces per day - Each cow produces 8 liters of milk per day.\n",
      "Step 3: Multiply the number of cows by the amount of milk each cow produces - 5 cows * 8 liters/cow = 40 liters.\n",
      "\n",
      "Therefore, the total amount of milk produced per day is 40 liters.\n",
      "\n",
      "Final Answer: 40 liters\n"
     ]
    }
   ],
   "source": [
    "res = graph.invoke({\"query\":\"A farmer has 5 cows, each produces 8 liters of milk per day. How much milk in total per day?\", \"prompt\": \"COT\"})\n",
    "print(res[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ed29ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

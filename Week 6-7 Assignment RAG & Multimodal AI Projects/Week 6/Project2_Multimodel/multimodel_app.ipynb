{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5bc35b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY=os.environ.get(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7c77ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from unstructured.partition.pdf import partition_pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b18fb528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_elements(file_path):\n",
    "    \"\"\"Extract text, table, and image elements from PDF\"\"\"\n",
    "    elements = partition_pdf(\n",
    "        filename=file_path,\n",
    "        strategy=\"hi_res\",\n",
    "        infer_table_structure=True,\n",
    "        extract_image_block_types=[\"Image\"],\n",
    "        extract_image_block_to_payload=True\n",
    "    )\n",
    "    print(f\"âœ… Extracted {len(elements)} elements\")\n",
    "    return elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f224eee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(elements):\n",
    "    \"\"\"Combine text from all elements and split into manageable chunks\"\"\"\n",
    "    all_text = \"\\n\".join([str(e) for e in elements])\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=300,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \";\"]\n",
    "    )\n",
    "    chunks = splitter.split_text(all_text)\n",
    "    print(f\"âœ… Created {len(chunks)} text chunks\")\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "66f111a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorstore(chunks):\n",
    "    \"\"\"Create embeddings using Gemini and store in Chroma\"\"\"\n",
    "    \n",
    "    #embeddings = embeddings  # or \"models/gemini-embedding-001\"\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "    vectorstore = Chroma.from_texts(chunks, embedding=embeddings, collection_name=\"pdf_chunks\")\n",
    "    print(\"âœ… Vector store created with Chroma\")\n",
    "    return vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "65164ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qa_chain(vectorstore):\n",
    "    \"\"\"Build a Retrieval-QA chain using Gemini 2.5 Flash\"\"\"\n",
    "    llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        chain_type=\"stuff\",\n",
    "        verbose=True\n",
    "    )\n",
    "    return qa_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "36c76cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "âœ… Extracted 173 elements\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Extract PDF elements\n",
    "elements = extract_elements(\"attention-is-all-you-need-Paper.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "373f1437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created 18 text chunks\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Chunk text\n",
    "chunks = create_chunks(elements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3accd678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Vector store created with Chroma\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 3: Create vector store\n",
    "vectorstore = create_vectorstore(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0018abc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Build QA chain\n",
    "qa_chain = create_qa_chain(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0df9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ðŸ’¬ Answer: The provided document is a list of academic paper citations. Based on the titles and authors, the main topics discussed across these papers revolve around:\n",
      "\n",
      "*   **Neural Networks and Architectures:** Especially Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks.\n",
      "*   **Attention Mechanisms:** Various approaches to attention, structured attention, and self-attention, often in the context of sequence-to-sequence models.\n",
      "*   **Natural Language Processing (NLP) Tasks:** Such as Neural Machine Translation (NMT), language modeling, and abstractive summarization.\n",
      "*   **Optimization and Training of Neural Networks:** Including methods like Adam and techniques to address issues like long-term dependencies and overfitting (e.g., Dropout).\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Ask questions\n",
    "query = \"What is the main topic discussed in the document?\"\n",
    "response = qa_chain.run(query)\n",
    "print(\"ðŸ’¬ Answer:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0b004d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ðŸ’¬ Answer: The Transformer model utilizes an encoder-decoder structure, which includes an encoder stack and a decoder stack.\n",
      "\n",
      "Here's the definition of each:\n",
      "\n",
      "*   **Encoder Stack:**\n",
      "    *   **Function:** Maps an input sequence of symbol representations (x1,...,xn) to a sequence of continuous representations z = (z1,...,zn).\n",
      "    *   **Structure:** Composed of a stack of N = 6 identical layers.\n",
      "    *   **Each Layer's Components:**\n",
      "        1.  A multi-head self-attention mechanism.\n",
      "        2.  A simple, position-wise fully connected feed-forward network.\n",
      "    *   **Self-attention in Encoder:** Each position in the encoder can attend to all positions in the previous layer of the encoder.\n",
      "\n",
      "*   **Decoder Stack:**\n",
      "    *   **Function:** Given the continuous representations `z` from the encoder, the decoder generates an output sequence (y1,...,ym) of symbols one element at a time. It is auto-regressive, meaning it consumes previously generated symbols as additional input when generating the next.\n",
      "    *   **Structure:** Also uses stacked self-attention and point-wise, fully connected layers, similar to the encoder.\n",
      "    *   **Each Layer's Components:**\n",
      "        1.  Attention sub-layers (specifically, masked multi-head self-attention).\n",
      "        2.  A fully connected feed-forward network.\n",
      "    *   **Self-attention in Decoder:** Each position in the decoder's self-attention layer can attend to all positions in the decoder up to and including that position. To preserve the auto-regressive property and prevent leftward information flow, illegal connections (future positions) are masked out in the input of the softmax.\n"
     ]
    }
   ],
   "source": [
    "query = \"Define the enccode and decoder\"\n",
    "response = qa_chain.run(query)\n",
    "print(\"ðŸ’¬ Answer:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "47b96c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ðŸ’¬ Answer: Figure 1 illustrates **The Transformer model architecture**.\n",
      "\n",
      "It depicts the overall structure of the Transformer, which is composed of an encoder-decoder architecture.\n",
      "*   The **left half** of the figure shows the **encoder**, which consists of a stack of N identical layers. Each encoder layer has two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network.\n",
      "*   The **right half** of the figure shows the **decoder**, also composed of a stack of N identical layers. Each decoder layer has three sub-layers: a masked multi-head self-attention mechanism, a multi-head attention mechanism that attends over the encoder's output, and a position-wise fully connected feed-forward network.\n",
      "\n",
      "Both the encoder and decoder utilize residual connections and layer normalization around their sub-layers. The figure also indicates the use of positional encoding for both input and output embeddings.\n"
     ]
    }
   ],
   "source": [
    "query = \"Define the figure 1: The Transformer- model architecture.\"\n",
    "response = qa_chain.run(query)\n",
    "print(\"ðŸ’¬ Answer:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
